from ktb_prompts import *
from ktb_settings import *
from ktb_func import *

import os
from typing import List, Dict, Optional, Any, Generator, Union
from pathlib import Path
import logging
import tiktoken
import aiofiles
from itertools import tee

"""**FUNCTION FOR CHAT**"""
# Configure logging
logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)


def search_file(repo_path, filename):
    repo_path = Path(repo_path)

    # ê²½ë¡œ ì•ˆì˜ ëª¨ë“  íŒŒì¼ê³¼ í´ë”ë¥¼ íƒìƒ‰
    for file_path in repo_path.rglob(filename):
        return file_path  # ì²« ë²ˆì§¸ë¡œ ì°¾ì€ íŒŒì¼ ë°˜í™˜

    return None  # íŒŒì¼ì´ ì—†ìœ¼ë©´ None ë°˜í™˜


async def process_file(file_path: Path, vector_store, file_metadata, chunk_id_counter: int) -> int:
    """íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€"""
    try:
        async with aiofiles.open(file_path, 'r', encoding='utf-8') as file:
            doc = await file.read()
            if doc.strip():
                if file_path.suffix == '.md':
                    chunks = embedding_chunker.chunk(doc)
                    if chunks:
                        chunk_contents = [
                            chunk.text.replace('\n', ' ').strip()
                            for chunk in chunks if chunk.text.strip()
                        ]
                        if chunk_contents:
                            chunk_ids = [
                                f"{file_path}_{i}"
                                for i in range(chunk_id_counter, chunk_id_counter + len(chunk_contents))
                            ]
                            chunk_metadatas = [
                                file_metadata for _ in range(len(chunk_contents))]
                            vector_store.add(
                                documents=chunk_contents,
                                metadatas=chunk_metadatas,
                                ids=chunk_ids
                            )
                            # print(f"Successfully processed file: {
                            #       file_metadata["filename"]} - Added {len(chunks)} chunks")
                            return len(chunk_contents)
                else:
                    if len(tiktoken.encoding_for_model(EMBEDDING_MODEL).encode(doc)) <= 8191:
                        # print(f"file: {file_metadata["filename"]} len: {
                        #       len(tiktoken.encoding_for_model(EMBEDDING_MODEL).encode(doc))}")
                        vector_store.add(
                            documents=[doc.replace('\n', ' ').strip()],
                            metadatas=[file_metadata],
                            ids=[f"{file_path}"]
                        )
                        # print(f"Successfully processed file: {
                        #       file_metadata["filename"]}")
                        return 1
                    else:
                        # print(f"will chunk file: {file_metadata["filename"]} len: {
                        #       len(tiktoken.encoding_for_model(EMBEDDING_MODEL).encode(doc))}")
                        max_chunk_size = 8192
                        overlap_size = 100
                        chunks = [
                            doc[i:i + max_chunk_size]
                            for i in range(0, len(doc), max_chunk_size - overlap_size)
                        ]
                        chunk_ids = [
                            f"{file_path}_{i}"
                            for i in range(chunk_id_counter, chunk_id_counter + len(chunks))
                        ]
                        chunk_metadatas = [
                            file_metadata for _ in range(len(chunks))]
                        vector_store.add(
                            documents=chunks,
                            metadatas=chunk_metadatas,
                            ids=chunk_ids
                        )
                        print(f"Successfully processed file: {
                              file_metadata["filename"]} - Added {len(chunks)} chunks")
                        return len(chunks)
    except UnicodeDecodeError as e:
        logger.error(f"Unicode decode error in file {file_path}: {str(e)}")
    except Exception as e:
        logger.error(f"Error processing file {file_path}: {str(e)}")
    return 0


async def add_data_to_db(db_name: str, path: str, file_type: List[str]) -> int:
    """DBì— ë°ì´í„°ë¥¼ ì¶”ê°€"""
    try:
        vector_store = chroma_client.get_or_create_collection(
            name=db_name,
            embedding_function=embedding_function,
            metadata=DISTANCE
        )
        repo_path = Path(path)
        total_files_processed = 0
        chunk_id_counter = 0
        for root, dirs, files in os.walk(repo_path):
            for filename in files:
                if filename == '.DS_Store':
                    continue
                if any(filename.endswith(ft) or filename == ft for ft in file_type):
                    file_path = search_file(repo_path, filename)
                    if file_path and file_path.exists() and file_path.is_file():
                        file_metadata = {
                            "filename": filename,
                            "path": str(file_path),
                            "repository": db_name
                        }
                        chunks_added = await process_file(file_path, vector_store, file_metadata, chunk_id_counter)
                        chunk_id_counter += chunks_added
                        if chunks_added > 0:
                            total_files_processed += 1
        if total_files_processed == 0:
            logger.error("No valid files were processed")
            return 0

        total_chunks = vector_store.count()
        print(f"Successfully processed {total_files_processed} files with total {
              total_chunks} chunks in {db_name}")
        return total_chunks
    except Exception as e:
        logger.error(f"Error adding data to DB: {str(e)}")
        raise


def db_search(query: str, db: Any, n_results: int = 3) -> Dict[str, Any]:
    """
    Search the vector database for similar documents.

    Args:
        query (str): Search query
        db: ChromaDB collection
        n_results (int): Number of results to return
        threshold (float): Similarity threshold

    Returns:
        Dict[str, Any]: Search results including documents and metadata
    """
    try:
        results = db.query(
            query_texts=query,
            n_results=n_results
        )
        return results

    except Exception as e:
        logger.error(f"Error searching DB: {str(e)}")
        raise


def generate_response(query: str, db_list: List[Any], chat_history: Optional[List[dict]] = None, augmented_query: Optional[str] = None, stream: bool = False) -> str:
    """Generate a response using LLM based on retrieved documents."""
    try:
        if augmented_query:
            retrieved_docs_source = db_search(augmented_query, db_list[0])
            retrieved_docs_generated = db_search(augmented_query, db_list[1])
        else:
            retrieved_docs_source = db_search(query, db_list[0])
            retrieved_docs_generated = db_search(query, db_list[1])

        system_prompt = CHATBOT_PROMPT
        user_prompt = f"""
Retrieved Content:
{retrieved_docs_source['documents']}

{retrieved_docs_generated['documents']}

User Query / Instruct: {query}
"""
        full_prompt = [{"role": "system", "content": system_prompt}]
        if chat_history:
            full_prompt.extend(chat_history)
        full_prompt.append({"role": "user", "content": user_prompt})

        client_gpt = get_openai_client()
        response = client_gpt.chat.completions.create(
            model=GPT_MODEL,
            messages=full_prompt,
            temperature=0.52,
            stream=False  # í•­ìƒ ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
        )

        return response.choices[0].message.content

    except Exception as e:
        logger.error(f"Error generating response: {str(e)}")
        raise


def stream_response(query: str, db_list: List[Any], chat_history: Optional[List[dict]] = None, augmented_query: Optional[str] = None) -> Generator:
    """Generate a streaming response."""
    try:
        if augmented_query:
            retrieved_docs_source = db_search(augmented_query, db_list[0])
            retrieved_docs_generated = db_search(augmented_query, db_list[1])
        else:
            retrieved_docs_source = db_search(query, db_list[0])
            retrieved_docs_generated = db_search(query, db_list[1])

        system_prompt = CHATBOT_PROMPT
        user_prompt = f"""
Retrieved Content:
{retrieved_docs_source['documents']}

{retrieved_docs_generated['documents']}

User Query / Instruct: {query}
"""
        full_prompt = [{"role": "system", "content": system_prompt}]
        if chat_history:
            full_prompt.extend(chat_history)
        full_prompt.append({"role": "user", "content": user_prompt})

        client_gpt = get_openai_client()
        response = client_gpt.chat.completions.create(
            model=GPT_MODEL,
            messages=full_prompt,
            temperature=0.52,
            stream=True  # í•­ìƒ ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        )

        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                yield chunk.choices[0].delta.content

    except Exception as e:
        logger.error(f"Error generating streaming response: {str(e)}")
        raise


def evaluate_response(response: str) -> bool:
    """
    ì‘ë‹µì˜ ì ì ˆì„±ì„ í‰ê°€í•˜ëŠ” í•¨ìˆ˜.
    ì ì ˆí•˜ì§€ ì•Šìœ¼ë©´ Falseë¥¼ ë°˜í™˜í•˜ì—¬ ì¿¼ë¦¬ ì¦ê°•ì„ ìœ ë„í•©ë‹ˆë‹¤.

    Args:
        response (str): ìƒì„±ëœ ì‘ë‹µ

    Returns:
        bool: ì‘ë‹µì´ ì ì ˆí•˜ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False
    """
    # ê°„ë‹¨í•œ í‰ê°€ ê¸°ì¤€ ì˜ˆì‹œ: ì‘ë‹µì´ ë¹„ì–´ìˆê±°ë‚˜ íŠ¹ì • í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©´ False
    if not response or "Error" in response or "not found" in response:
        return False
    return True


def codebase_chat(query: str, repo_url: str, chat_history: List[dict] = None, stream: bool = False) -> Union[str, Generator]:
    """ì±„íŒ… ì‘ë‹µ ìƒì„±"""
    try:
        repo_name, _ = parse_repo_url(repo_url)
        vector_store_source = chroma_client.get_collection(
            name=f"{repo_name}_source",
            embedding_function=embedding_function
        )
        vector_store_generated = chroma_client.get_collection(
            name=f"{repo_name}_generated",
            embedding_function=embedding_function
        )
        db_list = [vector_store_source, vector_store_generated]

        if chat_history:
            total_content = ''.join([msg['content']
                                    for msg in chat_history]) + query
            total_tokens = len(tiktoken.encoding_for_model(
                GPT_MODEL).encode(total_content))
            last_message = chat_history[-1] if total_tokens > MAX_TOKEN_LENGTH else chat_history
        else:
            last_message = None

        augmented_query = query_augmentation(query, last_message)

        if stream:
            return stream_response(query, db_list, last_message, augmented_query)
        else:
            return generate_response(query, db_list, last_message, augmented_query, stream)

    except Exception as e:
        logger.error(f"Error in codebase chat: {str(e)}")
        raise


def query_augmentation(query: str, chat_history: Optional[List[dict]] = None) -> str:
    """
    ì£¼ì–´ì§„ ì¿¼ë¦¬ì™€ ì´ì „ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ì¶”ê°€ ê²€ìƒ‰ì´ í•„ìš”í•œ ì •ë³´ë¥¼ ì‹ë³„í•˜ê³  ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        query (str): ì‚¬ìš©ì ì…ë ¥ ì¿¼ë¦¬
        previous_query (str): ì´ì „ ì¿¼ë¦¬ ë‚´ìš©
        previous_response (str): ì´ì „ ì‘ë‹µ ë‚´ìš©
    Returns:
        str: ì¦ê°•ëœ ê²€ìƒ‰ ì¿¼ë¦¬
    """
    prompt = AUGMENTATION_PROMPT

    if chat_history and len(chat_history) >= 2:
        previous_query = chat_history[-2]['content']
        previous_response = chat_history[-1]['content']
    else:
        previous_query = ""
        previous_response = ""

    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": f"Previous Query: {
            previous_query}\nPrevious Response: {previous_response}\nQuery: {query}"}
    ]
    client_gpt = get_openai_client()
    augmented_query = client_gpt.chat.completions.create(
        model=GPT_MODEL,
        messages=messages,
        temperature=0.52,
        max_tokens=150,
    )

    return augmented_query.choices[0].message.content


def should_augment_query(response: str) -> bool:
    if "Error" in response or "not found" in response:
        return True
    return False
# # ì‚¬ìš© ì˜ˆì‹œ
# query = """how can i use this api? and how can i fix Error 404"""
# response = """To use the API, you can follow these general steps based on the provided context:

# ### Using the API

# 1. **Authentication**:
#    - Obtain an access token through OAuth by following the login process with your chosen OAuth provider (e.g., Kakao, Google).
#    - Use the generated access token for subsequent API requests.

# 2. **Making Requests**:
#    - Use tools like Postman or libraries like RestAssured (as shown in the `RecommendTripAcceptenceFixture` and `KeywordAcceptenceFixture` classes) to make HTTP requests to the API endpoints.
#    - Ensure you set the correct HTTP method (GET, POST, DELETE) and include the access token in the authorization header.

# 3. **Example Requests**:
#    - To select a preferred trip:
#      ```java
#      ExtractableResponse<Response> response = RecommendTripAcceptenceFixture.ì„ í˜¸_ì—¬í–‰ì§€ë¥¼_ì„ íƒí•œë‹¤(tripId, accessToken);
#      ```
#    - To retrieve AI-customized recommended trips:
#      ```java
#      ExtractableResponse<Response> response = RecommendTripAcceptenceFixture.AI_ë§ì¶¤_ì¶”ì²œ_ì—¬í–‰ì§€ë¥¼_ì¡°íšŒí•œë‹¤(accessToken);
#      ```

# ### Fixing Error 404

# A 404 error typically indicates that the requested resource could not be found. Here are some steps to troubleshoot and fix this error:

# 1. **Check the Endpoint**: Ensure that you are using the correct URL for the API endpoint. Refer to the API documentation or the provided context to verify the endpoint paths.

# 2. **Verify HTTP Method**: Make sure you are using the correct HTTP method (GET, POST, DELETE) as required by the endpoint.

# 3. **Access Token**: Ensure that your access token is valid and has not expired. If necessary, refresh the token.

# 4. **Server Status**: Check if the server is running and accessible. If you are running the backend locally, ensure that it is started correctly.

# 5. **Path Parameters**: If the endpoint requires path parameters (like `tripId` or `oAuthProvider`), ensure they are correctly formatted and provided in the request.

# 6. **Logs and Debugging**: Check server logs for any additional error messages that might provide more context on why the resource was not found.

# By following these steps, you should be able to effectively use the API and troubleshoot any 404 errors you encounter."""
# # context = """'package moheng.auth.domain;  import static org.junit.jupiter.api.Assertions.*;  import moheng.auth.infrastructure.KakaoOAuthClient; import org.junit.jupiter.api.DisplayName; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest;  @SpringBootTest public class KakaoOAuthClientTest {     @Autowired     private KakaoOAuthClient kakaoOAuthClient;      @DisplayName("OAuth í´ë¼ì´ì–¸íŠ¸ ì‹ë³„ìê°€ ë™ì¼í•˜ë‹¤ë©´ ì°¸ì„ ë¦¬í„´í•œë‹¤.")     @Test     void OAuth_í´ë¼ì´ì–¸íŠ¸_ì‹ë³„ìê°€_ë™ì¼í•˜ë‹¤ë©´_ì°¸ì„_ë¦¬í„´í•œë‹¤() {         // given, when, then         assertTrue(kakaoOAuthClient.isSame("KAKAO"));     } }', 'package moheng.acceptance.fixture;  import io.restassured.RestAssured; import io.restassured.response.ExtractableResponse; import io.restassured.response.Response; import moheng.auth.dto.AccessTokenResponse; import moheng.keyword.dto.KeywordCreateRequest; import moheng.keyword.dto.TripsByKeyWordsRequest; import org.springframework.http.HttpStatus; import org.springframework.http.MediaType;  public class KeywordAcceptenceFixture {      public static ExtractableResponse<Response> ëª¨ë“ _í‚¤ì›Œë“œë¥¼_ì°¾ëŠ”ë‹¤(final AccessTokenResponse accessTokenResponse) {         return RestAssured.given().log().all()                 .auth().oauth2(accessTokenResponse.getAccessToken())                 .contentType(MediaType.APPLICATION_JSON_VALUE)                 .when().get("/api/keyword")                 .then().log().all()                 .statusCode(HttpStatus.OK.value())                 .extract();     }      public static ExtractableResponse<Response> í‚¤ì›Œë“œë¥¼_ìƒì„±í•œë‹¤(final String name) {         return RestAssured.given().log().all()                 .contentType(MediaType.APPLICATION_JSON_VALUE)                 .body(new KeywordCreateRequest(name))                 .when().post("/api/keyword")                 .then().log().all()                 .statusCode(org.springframework.http.HttpStatus.NO_CONTENT.value())                 .extract();     }      public static ExtractableResponse<Response> í‚¤ì›Œë“œ_ë¦¬ìŠ¤íŠ¸ë¡œ_ì—¬í–‰ì§€ë¥¼_ì¶”ì²œë°›ëŠ”ë‹¤(final String accessToken, final TripsByKeyWordsRequest tripsByKeyWordsRequest) {         return RestAssured.given().log().all()                 .auth().oauth2(accessToken)                 .contentType(MediaType.APPLICATION_JSON_VALUE)                 .body(tripsByKeyWordsRequest)                 .when().post("/api/keyword/trip/recommend")                 .then().log().all()                 .statusCode(HttpStatus.OK.value())                 .extract();     }      public static ExtractableResponse<Response> ëœë¤_í‚¤ì›Œë“œ_ë¦¬ìŠ¤íŠ¸ë¡œ_ì—¬í–‰ì§€ë¥¼_ì¶”ì²œë°›ëŠ”ë‹¤() {         return RestAssured.given().log().all()                 .contentType(MediaType.APPLICATION_JSON_VALUE)                 .when().get("/api/keyword/random/trip")                 .then().log().all()                 .statusCode(HttpStatus.OK.value())                 .extract();     } }', 'package moheng.acceptance.fixture;  import io.restassured.RestAssured; import io.restassured.response.ExtractableResponse; import io.restassured.response.Response; import moheng.recommendtrip.dto.RecommendTripCreateRequest; import org.springframework.http.HttpStatus; import org.springframework.http.MediaType;  public class RecommendTripAcceptenceFixture {     public static ExtractableResponse<Response> ì„ í˜¸_ì—¬í–‰ì§€ë¥¼_ì„ íƒí•œë‹¤(final long tripId, final String accessToken) {         return RestAssured.given().log().all()                 .contentType(MediaType.APPLICATION_JSON_VALUE)                 .auth().oauth2(accessToken)                 .body(new RecommendTripCreateRequest(tripId))                 .when().post("/api/recommend")                 .then().log().all()                 .statusCode(org.springframework.http.HttpStatus.NO_CONTENT.value())                 .extract();     }      public static ExtractableResponse<Response> AI_ë§ì¶¤_ì¶”ì²œ_ì—¬í–‰ì§€ë¥¼_ì¡°íšŒí•œë‹¤(final String accessToken) {         return RestAssured.given().log().all()                 .contentType(MediaType.APPLICATION_JSON_VALUE)                 .auth().oauth2(accessToken)                 .when().get("/api/recommend")                 .then().log().all()                 .statusCode(HttpStatus.OK.value())                 .extract();     } }']]

# # [['branch protect testì…ë‹ˆë‹¤. ì  í‚¨ìŠ¤ì˜ ë¹Œë“œê°€ ì„±ê³µí–ˆì„ë•Œ, í˜¹ì€ ì‹¤íŒ¨í–ˆì„ ë•Œ ë¨¸ì§€ê°€ ë¸”ë¡ë˜ê±°ë‚˜ í—ˆìš©ë˜ë©´ ì„±ê³µì…ë‹ˆë‹¤. ì´ ë‘ê°€ì§€ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.  1. ì˜ë„ì ìœ¼ë¡œ ì‹¤íŒ¨ì‹œì¼œì„œ ë¨¸ì§€ ë¸”ë¡œí‚¹ í™•ì¸í•˜ê¸° 2. ì˜ë„ì ìœ¼ë¡œ ì„±ê³µì‹œí‚¨í›„ ë¨¸ì§€ í™•ì¸í•˜ê¸°', '# Moheng  ## ğŸ–¼ Preview ![Preview Image](link_to_your_preview_image)  ## Table of Contents  - [Overview](#-overview) - [Analysis](#-analysis) - [Project Structure](#-project-structure) - [Getting Started](#-getting-started)   - [Prerequisites](#prerequisites)   - [Installation](#installation)   - [Usage](#usage)  ## ğŸ“ Overview ì´ í”„ë¡œì íŠ¸ëŠ” ì—¬í–‰ ê³„íš ë° ì¶”ì²œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. - ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ì€ ì‚¬ìš©ìì—ê²Œ ë§ì¶¤í˜• ì—¬í–‰ ì¶”ì²œì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  ### Main Purpose - ì‚¬ìš©ìì˜ ì„ í˜¸ë„ì™€ í´ë¦­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—¬í–‰ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. - ì—¬í–‰ ê³„íšì„ ì„¸ìš°ëŠ” ë° ë„ì›€ì„ ì£¼ë©°, ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. - ì—¬í–‰ì„ ì¢‹ì•„í•˜ëŠ” ì‚¬ìš©ìë“¤ì„ ì£¼ìš” ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.  ### Key Features - ì†Œì…œ ë¡œê·¸ì¸ ê¸°ëŠ¥ (Kakao, Google) - ì‚¬ìš©ì ë§ì¶¤í˜• ì—¬í–‰ ì¶”ì²œ - ì—¬í–‰ ì¼ì • ê´€ë¦¬ ê¸°ëŠ¥ - ì‹¤ì‹œê°„ ì—¬í–‰ ì •ë³´ ì œê³µ  ### Core Technology Stack - Frontend: React, Vite - Backend: Spring Boot - Database: MySQL - ê¸°íƒ€: FastAPI, Python  ## ğŸ“Š Analysis - ë°ì´í„° ë¶„ì„ ê²°ê³¼: ì‚¬ìš©ì í´ë¦­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ - ì„±ëŠ¥ ë©”íŠ¸ë¦­: ì¶”ì²œì˜ ì •í™•ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ - ì£¼ìš” ì¸ì‚¬ì´íŠ¸: ì‚¬ìš©ì ì„ í˜¸ë„ì— ë”°ë¼ ì¶”ì²œì˜ ì§ˆì´ í–¥ìƒë¨  ## ğŸ“ Project Structure ``` moheng â”œâ”€â”€ ğŸ“ ai â”‚   â”œâ”€â”€ ğŸ“ model_serving â”‚   â”‚   â”œâ”€â”€ ğŸ“ application â”‚   â”‚   â”œâ”€â”€ ğŸ“ domain â”‚   â”‚   â”œâ”€â”€ ğŸ“ infra â”‚   â”‚   â””â”€â”€ ğŸ“ interface â”‚   â””â”€â”€ ... â”œâ”€â”€ ğŸ“ frontend â”‚   â”œâ”€â”€ ğŸ“ src â”‚   â”‚   â”œâ”€â”€ ğŸ“ api â”‚   â”‚   â”œâ”€â”€ ğŸ“ components â”‚   â”‚   â””â”€â”€ ğŸ“ pages â”‚   â””â”€â”€ ... â”œâ”€â”€ ğŸ“ backend â”‚   â”œâ”€â”€ ğŸ“ moheng â”‚   â”‚   â”œâ”€â”€ ğŸ“ auth â”‚   â”‚   â”œâ”€â”€ ğŸ“ member â”‚   â”‚   â”œâ”€â”€ ğŸ“ planner â”‚   â”‚   â””â”€â”€ ğŸ“ trip â”‚   â””â”€â”€ ... â””â”€â”€ ... ```  ## ğŸš€ Getting Started ### Prerequisites - Docker - Java 22 - Python 3.11 - Node.js  ### Installation ```bash # ë ˆí¬ì§€í† ë¦¬ í´ë¡  git clone https://github.com/kakao-25/moheng.git  # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ cd frontend npm install  cd ../backend ./gradlew build  cd ../ai pip install poetry poetry install ```  ### Usage ```bash # í”„ë¡ íŠ¸ì—”ë“œ ì‹¤í–‰ cd frontend npm start  # ë°±ì—”ë“œ ì‹¤í–‰ cd backend ./gradlew bootRun  # AI ëª¨ë¸ ì‹¤í–‰ cd ai poetry run python main.py ```', '# Moheng  ## ğŸ–¼ Preview ![Preview Image](link_to_your_preview_image)  ## Table of Contents  - [Overview](#-overview) - [Analysis](#-analysis) - [Project Structure](#-project-structure) - [Getting Started](#-getting-started)   - [Prerequisites](#prerequisites)   - [Installation](#installation)   - [Usage](#usage)  ## ğŸ“ Overview ì´ í”„ë¡œì íŠ¸ëŠ” ì—¬í–‰ ê³„íš ë° ì¶”ì²œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. - í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©ì ì€ ì‚¬ìš©ìì—ê²Œ ë§ì¶¤í˜• ì—¬í–‰ ì¶”ì²œì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.  ### Main Purpose - ì‚¬ìš©ìì˜ ì„ í˜¸ë„ì™€ í´ë¦­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—¬í–‰ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤. - ì—¬í–‰ ê³„íšì„ ì„¸ìš°ëŠ” ë° ë„ì›€ì„ ì£¼ë©°, ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. - ì—¬í–‰ì„ ì¢‹ì•„í•˜ëŠ” ì‚¬ìš©ìë“¤ì„ ì£¼ìš” ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.  ### Key Features - ì†Œì…œ ë¡œê·¸ì¸ ê¸°ëŠ¥ (Kakao, Google) - ì‚¬ìš©ì ë§ì¶¤í˜• ì—¬í–‰ ì¶”ì²œ - ì—¬í–‰ ì¼ì • ê´€ë¦¬ ê¸°ëŠ¥ - ì‹¤ì‹œê°„ ì—¬í–‰ ì •ë³´ ì œê³µ  ### Core Technology Stack - Frontend: React, Vite - Backend: Spring Boot - Database: MySQL - ê¸°íƒ€: FastAPI, Python  ## ğŸ“Š Analysis - ë°ì´í„° ë¶„ì„ ê²°ê³¼: ì‚¬ìš©ì í´ë¦­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ - ì„±ëŠ¥ ë©”íŠ¸ë¦­: ì¶”ì²œì˜ ì •í™•ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ - ì£¼ìš” ì¸ì‚¬ì´íŠ¸: ì‚¬ìš©ì ì„ í˜¸ë„ì— ë”°ë¼ ì¶”ì²œì˜ ì§ˆì´ í–¥ìƒë¨  ## ğŸ“ Project Structure ``` moheng â”œâ”€â”€ ğŸ“ ai â”‚   â”œâ”€â”€ ğŸ“ model_serving â”‚   â”‚   â”œâ”€â”€ ğŸ“ application â”‚   â”‚   â”œâ”€â”€ ğŸ“ domain â”‚   â”‚   â”œâ”€â”€ ğŸ“ infra â”‚   â”‚   â””â”€â”€ ğŸ“ interface â”‚   â””â”€â”€ ... â”œâ”€â”€ ğŸ“ frontend â”‚   â”œâ”€â”€ ğŸ“ src â”‚   â”‚   â”œâ”€â”€ ğŸ“ api â”‚   â”‚   â”œâ”€â”€ ğŸ“ components â”‚   â”‚   â””â”€â”€ ğŸ“ pages â”‚   â””â”€â”€ ... â”œâ”€â”€ ğŸ“ backend â”‚   â”œâ”€â”€ ğŸ“ moheng â”‚   â”‚   â”œâ”€â”€ ğŸ“ auth â”‚   â”‚   â”œâ”€â”€ ğŸ“ member â”‚   â”‚   â”œâ”€â”€ ğŸ“ planner â”‚   â”‚   â””â”€â”€ ğŸ“ trip â”‚   â””â”€â”€ ... â””â”€â”€ ... ```  ## ğŸš€ Getting Started ### Prerequisites - Docker - Java 22 - Python 3.11 - Node.js  ### Installation ```bash # ë ˆí¬ì§€í† ë¦¬ í´ë¡  git clone https://github.com/kakao-25/moheng.git  # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ cd frontend npm install  cd ../backend ./gradlew build  cd ../ai pip install poetry poetry install ```  ### Usage ```bash # í”„ë¡ íŠ¸ì—”ë“œ ì‹¤í–‰ cd frontend npm start  # ë°±ì—”ë“œ ì‹¤í–‰ cd backend ./gradlew bootRun  # AI ëª¨ë¸ ì‹¤í–‰ cd ai poetry run python main.py ```'"""
# prompt = query_augmentation(query, response)
# print(prompt)
