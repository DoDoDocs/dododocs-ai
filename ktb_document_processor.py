"""ë¬¸ì„œ ì²˜ë¦¬ ê´€ë ¨ ì½”ë“œ"""
from dataclasses import dataclass, field
from typing import List, Any, Optional, Set, Dict
import asyncio
import logging
import re
import os
import time
import aiofiles

from ktb_api_client import APIClient
from ktb_prompts import *
from ktb_settings import *
from ktb_func import *
# logger ì„¤ì • ì¶”ê°€
logger = logging.getLogger(__name__)


@dataclass
class SourceFileInfo:
    """ì†ŒìŠ¤ íŒŒì¼ì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    package: Optional[str] = None      # íŒ¨í‚¤ì§€/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ (ì˜ˆ: com.example.project)
    imports: Set[str] = field(default_factory=set)  # import/using/require ë¬¸ ì§‘í•©
    class_name: str = ""              # í´ë˜ìŠ¤/í•¨ìˆ˜ëª…
    content: str = ""                # íŒŒì¼ ì „ì²´ ë‚´ìš©
    file_type: str = ""             # íŒŒì¼ íƒ€ì… (java, py, js ë“±)

    def __post_init__(self):
        """ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ë° ê¸°ë³¸ê°’ ì„¤ì •"""
        if self.imports is None:
            self.imports = set()

        if self.class_name is None:
            self.class_name = ""

        if self.content is None:
            self.content = ""

        if self.file_type is None:
            self.file_type = ""

    def __str__(self) -> str:
        """ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¬¸ìì—´ í‘œí˜„"""
        return (
            f"SourceFileInfo(\n"
            f"  package: {self.package}\n"
            f"  class_name: {self.class_name}\n"
            f"  file_type: {self.file_type}\n"
            f"  imports: {len(self.imports)} items\n"
            f"  content: {len(self.content)} chars\n"
            f")"
        )


class DocumentProcessor:
    """ë¬¸ì„œ ìƒì„± ë° ì²˜ë¦¬"""

    def __init__(self, api_client: APIClient):
        self.api_client = api_client
        self.source_extensions = {
            '.java': 'java',
            '.py': 'py',
            '.js': 'js',
            '.ts': 'ts',
            '.cpp': 'cpp',
            '.cs': 'cs',
            '.go': 'go'
        }
        self.parsers = {
            'java': self._parse_java_file,
            'py': self._parse_python_file,
            'js': self._parse_javascript_file,
            'ts': self._parse_typescript_file,
            'cpp': self._parse_cpp_file,
            'cs': self._parse_cs_file
        }
        self.ENDPOINT = f"https://generativelanguage.googleapis.com/v1beta/models/{MODEL}:generateContent"
        self.PARAMS = {
            "key": os.getenv("GOOGLE_API_KEY")  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
            }
        self.HEADERS = {
            "Content-Type": "application/json",
        }

    def _parse_source_file(self, content: str, file_type: str) -> SourceFileInfo:
        """ì†ŒìŠ¤ íŒŒì¼ íŒŒì‹±"""
        if file_type in self.parsers:
            return self.parsers[file_type](content)
        return self._parse_generic_file(content, file_type)

    def _parse_java_file(self, content: str) -> SourceFileInfo:
        """Java íŒŒì¼ íŒŒì‹±"""
        imports = set(re.findall(r'^import\s+[\w.*]+;', content, re.MULTILINE))
        package_match = re.search(
            r'^package\s+([\w.]+);', content, re.MULTILINE)
        class_match = re.search(r'(?:class|interface|enum)\s+(\w+)', content)

        package = package_match.group(1) if package_match else None
        class_name = class_match.group(1) if class_match else ""

        return SourceFileInfo(
            package=package,
            imports=imports,
            class_name=class_name,
            content=content,
            file_type='java'
        )

    def _parse_python_file(self, content: str) -> SourceFileInfo:
        """Python íŒŒì¼ íŒŒì‹±"""
        imports = set(re.findall(
            r'^(?:from\s+[\w.]+\s+)?import\s+[\w.*,\s]+', content, re.MULTILINE))
        class_match = re.search(r'(?:class|def)\s+(\w+)', content)
        class_name = class_match.group(1) if class_match else ""

        return SourceFileInfo(
            package=None,
            imports=imports,
            class_name=class_name,
            content=content,
            file_type='py'
        )

    def _parse_javascript_file(self, content: str) -> SourceFileInfo:
        """JavaScript íŒŒì¼ íŒŒì‹±"""
        imports = set(re.findall(
            r'^(?:import|require)\s+.*?;?$', content, re.MULTILINE))
        class_match = re.search(r'(?:class|function)\s+(\w+)', content)
        class_name = class_match.group(1) if class_match else ""

        return SourceFileInfo(
            package=None,
            imports=imports,
            class_name=class_name,
            content=content,
            file_type='js'
        )

    def _parse_typescript_file(self, content: str) -> SourceFileInfo:
        """TypeScript íŒŒì¼ íŒŒì‹±"""
        imports = set(re.findall(
            r'^(?:import|require)\s+.*?;?$', content, re.MULTILINE))
        class_match = re.search(
            r'(?:class|interface|function)\s+(\w+)', content)
        class_name = class_match.group(1) if class_match else ""

        return SourceFileInfo(
            package=None,
            imports=imports,
            class_name=class_name,
            content=content,
            file_type='ts'
        )

    def _parse_cpp_file(self, content: str) -> SourceFileInfo:
        """C++ íŒŒì¼ íŒŒì‹±"""
        imports = set(re.findall(r'#include\s+[<"].*?[>"]', content))
        class_match = re.search(
            r'(?:class|struct|void|int|bool|char|float|double)\s+(\w+)', content)
        class_name = class_match.group(1) if class_match else ""

        return SourceFileInfo(
            package=None,
            imports=imports,
            class_name=class_name,
            content=content,
            file_type='cpp'
        )

    def _parse_cs_file(self, content: str) -> SourceFileInfo:
        """C# íŒŒì¼ íŒŒì‹±"""
        imports = set(re.findall(
            r'^using\s+([\w\.]+);', content, re.MULTILINE))
        class_match = re.search(r'(?:class|interface|struct)\s+(\w+)', content)
        class_name = class_match.group(1) if class_match else ""

        return SourceFileInfo(
            package=None,
            imports=imports,
            class_name=class_name,
            content=content,
            file_type='cs'
        )

    def _parse_generic_file(self, content: str, file_type: str) -> SourceFileInfo:
        """ê¸°ë³¸ íŒŒì„œ"""
        return SourceFileInfo(
            package=None,
            imports=set(),
            class_name="",
            content=content,
            file_type=file_type
        )

    async def process_readme(self, repo_url: str, clone_dir: str, readme_key: str, korean: bool, blocks: List[str], metadata: dict = None, max_retries: int = 1) -> List[Any]:
        """ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹¤í–‰"""
        tasks = []
        start_time = time.perf_counter()

        # README ìƒì„± íƒœìŠ¤í¬
        readme_task = asyncio.create_task(
            self._generate_readme(repo_url, clone_dir, korean, blocks),
            name="readme_generation"
        )
        tasks.append(readme_task)

        usage_task = asyncio.create_task(
            self._generate_usage(repo_url, clone_dir, korean),
            name="usage_generation"
        )
        tasks.append(usage_task)

        # ëª¨ë“  íƒœìŠ¤í¬ ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜
        for attempt in range(max_retries):
            try:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                print(f"results length: {len(results)}")
                # ì˜¤ë¥˜ê°€ ë°œìƒí•œ íƒœìŠ¤í¬ ì‹ë³„ ë° ì¬ì‹œë„
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        print(
                            f"Task {tasks[i].get_name()} failed: {str(result)}")
                        if attempt < max_retries - 1:
                            logger.info(f"Retrying task {tasks[i].get_name()} (attempt {
                                        attempt + 1}/{max_retries})")
                            if tasks[i].get_name() == "readme_generation":
                                tasks[i] = asyncio.create_task(
                                    self._generate_readme(
                                        repo_url, clone_dir, korean, blocks),
                                    name="readme_generation"
                                )
                            elif tasks[i].get_name() == "usage_generation":
                                tasks[i] = asyncio.create_task(
                                    self._generate_usage(
                                        repo_url, clone_dir, korean),
                                    name="usage_generation"
                                )
                            break  # ì¬ì‹œë„ í›„ ë‹¤ì‹œ gather ì‹¤í–‰
                else:
                    # ëª¨ë“  íƒœìŠ¤í¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œëœ ê²½ìš°
                    if len(results) > 1 and isinstance(results[0], str) and isinstance(results[1], str):
                        merged_content = self._update_readme_with_usage(
                            results[0], results[1])
                        await self._save_readme(merged_content, clone_dir, readme_key, metadata)
                    elif isinstance(results[0], str):
                        await self._save_readme(results[0], clone_dir, readme_key, metadata)

                    end_time = time.perf_counter()
                    print(f"README ë° Usage ìƒì„± ì™„ë£Œ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time} ì´ˆ")
                    return results

            except Exception as e:
                print(f"Task execution failed: {str(e)}")
                if attempt >= max_retries - 1:
                    raise
                else:
                    logger.info(f"Retrying all tasks (attempt {
                                attempt + 1}/{max_retries})")
                    await asyncio.sleep(2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„

        raise Exception("All retries failed for process_readme")

    def get_optimized_source_files(self, repo_dir: str) -> Dict[str, List[SourceFileInfo]]:
        """ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ ìµœì í™”í•˜ì—¬ ì €ì¥"""
        package_map = {}
        source_extensions = self.source_extensions
        for root, _, files in os.walk(repo_dir):
            # if any(excl in root for excl in EXCLUDE_DIRS):
            #     continue

            for file in files:
                ext = os.path.splitext(file)[1].lower()
                if ext not in source_extensions:
                    continue

                file_path = os.path.join(root, file)
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        file_info = self._parse_source_file(
                            content,
                            source_extensions[ext]
                        )
                        # íŒ¨í‚¤ì§€/ëª¨ë“ˆë³„ë¡œ ë¶„ë¥˜
                        key = file_info.package or os.path.dirname(file_path)
                        if key not in package_map:
                            package_map[key] = []
                        package_map[key].append(file_info)
                except Exception as e:
                    print(f"íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜ ({file_path}): {str(e)}")

        return package_map

    async def _generate_readme(self, repo_url: str, clone_dir: str, korean: bool, blocks: List[str]) -> Optional[str]:
        """README ìƒì„±"""
        readme_template = generate_readme_prompt(blocks, korean)
        start_time = time.perf_counter()
        try:
            source_files = self.get_optimized_source_files(clone_dir)
            if not source_files:
                print("ì†ŒìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
            optimized_context = self._build_optimized_context(source_files)
            chunks = chunker.chunk(optimized_context)
            logger.info(f"chunks length : {len(chunks)}")
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ ë³‘í•©
            result = await self._process_chunks(chunks, repo_url, readme_template, korean)

            logger.info(f"README ìƒì„± ì™„ë£Œ ì²˜ë¦¬ ì‹œê°„ : {
                        time.perf_counter() - start_time} ì´ˆ")
            return result

        except Exception as e:
            print(f"README generation failed: {str(e)}")
            return None

    def _get_build_files(self, repo_dir: str) -> List[str]:
        """ë¹Œë“œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        build_files = []

        for root, _, files in os.walk(repo_dir):
            if any(excl in root for excl in EXCLUDE_DIRS):
                continue
            # node_modules ë””ë ‰í† ë¦¬ ì²´í¬
            if "node_modules" in root:
                continue

            for file in files:
                file_path = os.path.join(root, file)

                # ì¼ë°˜ ë¹Œë“œ íŒŒì¼ ì²´í¬
                if file.endswith(tuple(BUILD_FILE_NAMES)) or any(name in file for name in BUILD_FILE_NAMES):
                    print(f"ë¹Œë“œ íŒŒì¼ ë°œê²¬: {file}")
                    build_files.append(file_path)

        if not build_files:
            print(f"ë¹Œë“œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {repo_dir}")

        return build_files

    def _build_files_context(self, build_files: List[str], clone_dir: str) -> str:
        """ë¹Œë“œ íŒŒì¼ë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        for file_path in build_files:
            try:
                rel_path = os.path.relpath(file_path, clone_dir)
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    context_parts.append(
                        f"FILE_PATH: {rel_path}\nFILE_CONTENT: {
                            content}\nFILE_END\n\n"
                    )
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({rel_path}): {str(e)}")
                continue
        return "".join(context_parts)

    async def _process_chunks(self, chunks: List[str], repo_url: str, prompt: str, korean: bool) -> Optional[str]:
        """ì²­í¬ ë¹„ë™ê¸° ì²˜ë¦¬"""
        tasks = [
                self.process_chunk(chunk.text, repo_url, prompt)
                for chunk in chunks
            ]
        chunk_summaries = await asyncio.gather(*tasks)  # ì´ ë¶€ë¶„ì„ ë¸”ë¡ ì•ˆìœ¼ë¡œ ì´ë™
        chunk_summaries = [s for s in chunk_summaries if s]  # None ê°’ í•„í„°ë§

        if not chunk_summaries:
            return None
        
        if len(chunk_summaries) > 1:
            start_time = time.perf_counter()
            messages = [{"role": "system", "content": prompt}]
            for summary in chunk_summaries:
                messages.append({"role": "user", "content": summary})
            if korean:
                messages.append({
                    "role": "user",
                    "content": f"Create a readme based on the previous information. MUST be in Korean. git repository url : {repo_url}"
                })
            else:
                messages.append({
                    "role": "user",
                    "content": f"Create a readme based on the previous information. git repository url : {repo_url}"
                })
            doc_response = self._get_completion(messages)
            end_time = time.perf_counter()
            print(f"README ìƒì„± ì™„ë£Œ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time} ì´ˆ")
            return doc_response
        else:
            return chunk_summaries[0]

    async def _generate_usage(self, repo_url: str, clone_dir: str, korean: bool) -> Optional[str]:
        """Usage ìƒì„±"""
        usage_template = generate_readme_prompt(
            ["START_BLOCK"], korean)
        start_time = time.perf_counter()
        try:
            last_slash_index = repo_url.rfind('/')
            repo_url = repo_url[:last_slash_index] + ".git"

            build_files = self._get_build_files(clone_dir)
            context = self._build_files_context(build_files, clone_dir)

            chunks = chunker.chunk(context)
            print(f"Split into {len(chunks)} chunks - USAGE")
            
            result = await self._process_chunks(chunks, repo_url, usage_template, korean)
            end_time = time.perf_counter()
            print(f"USAGE ìƒì„± ì™„ë£Œ ì²˜ë¦¬ ì‹œê°„: {end_time - start_time} ì´ˆ")
            return result

        except Exception as e:
            print(f"Usage generation failed: {str(e)}")
            return None

    def _get_code_contents(self, files: List[str]) -> List[str]:
        """íŒŒì¼ ë‚´ìš© ì½ê¸° ë° importëœ í´ë˜ìŠ¤ ë‚´ìš© ê²°í•©"""
        contents = []
        for file in files:
            total_code = ''
            path = os.path.dirname(file)

            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({file}): {str(e)}")
                contents.append("")
                continue

            imported_classes = self.extract_imported_classes(content)

            for class_name in imported_classes:
                class_path = class_name.replace('.', '/') + '.java'
                full_path = os.path.join(path, class_path)

                if os.path.exists(full_path):
                    try:
                        class_content = self.read_file_content(full_path)
                        total_code += f"Content of {
                            class_name} :\n{class_content}\n\n"
                        # print(f" + Content of {class_name}")
                    except Exception as e:
                        print(f"í´ë˜ìŠ¤ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({full_path}): {str(e)}")
                else:
                    logger.warning(f"í´ë˜ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {full_path}")

            total_code += f"code :\n{content}"
            contents.append(total_code)
        return contents

    def _extract_filename(self, filepath: str) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ"""
        return os.path.basename(filepath)

    def _build_optimized_context(self, package_map: Dict[str, List[SourceFileInfo]]) -> str:
        """ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context_parts = []

        for package, classes in package_map.items():
            # íŒ¨í‚¤ì§€ë³„ë¡œ ê³µí†µ importë¬¸ ì²˜ë¦¬
            common_imports = set.intersection(
                *[cls.imports for cls in classes])

            context_parts.append(f"\nPACKAGE: {package}")
            if common_imports:
                context_parts.append("COMMON IMPORTS:")
                context_parts.extend(sorted(common_imports))

            # ê° í´ë˜ìŠ¤ì˜ ê³ ìœ í•œ ë‚´ìš©ë§Œ í¬í•¨
            for cls in classes:
                unique_imports = cls.imports - common_imports
                class_context = [
                    f"\nCLASS: {cls.class_name}",
                    "UNIQUE IMPORTS:" if unique_imports else "",
                    "\n".join(sorted(unique_imports)),
                    cls.content
                ]
                context_parts.append("\n".join(filter(None, class_context)))

        return "\n".join(context_parts)

    def _get_completion(
        self,  # self ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
        messages: List[Dict],
        model: Optional[str] = MODEL,
        temperature: Optional[float] = TEMPERATURE,
        seed: Optional[int] = SEED,
    ) -> str:
        params = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "seed": seed
        }

        try:
            completion = OpenAI_client.chat.completions.create(**params)
            return completion.choices[0].message.content

        except Exception as e:
            print(f"Completion ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return "", None

    def _update_readme_with_usage(self, readme_content: str, usage_content: str) -> str:
        """README ë‚´ìš©ì— Usage ë‚´ìš©ì„ ë³‘í•©í•©ë‹ˆë‹¤.

        Args:
            readme_content (str): ê¸°ì¡´ README ë‚´ìš©
            usage_content (str): Usage ì„¹ì…˜ ë‚´ìš©

        Returns:
            str: ë³‘í•©ëœ README ë‚´ìš©
        """
        try:
            # ìˆ˜ì •ëœ ì •ê·œ í‘œí˜„ì‹: Getting Started ì„¹ì…˜ ì‹œì‘ë¶€í„° Motivation ì„¹ì…˜ê¹Œì§€
            pattern_readme = r"(## ğŸš€ Getting Started.*?)(?=## ğŸ’¡ Motivation|$)"
            pattern_usage = r"(## ğŸš€ Getting Started.*)"

            readme_match = re.search(pattern_readme, readme_content, re.DOTALL)
            usage_match = re.search(pattern_usage, usage_content, re.DOTALL)

            readme_getting_started = readme_match.group(0)
            usage_getting_started = usage_match.group(0) + "\n\n"

            # Getting Started ì„¹ì…˜ì„ usage_contentì˜ ì„¹ì…˜ìœ¼ë¡œ êµì²´
            new_content = readme_content.replace(
                readme_getting_started, usage_getting_started)

            return new_content

        except Exception as e:
            print(f"README ë³‘í•© ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return readme_content

    async def _save_readme(self, content: str, clone_dir: str, readme_key: str, metadata: dict = None):
        """README íŒŒì¼ ì €ì¥"""
        #readme_path = os.path.join(clone_dir, "README.md")
        readme_path = "./README.md"
        try:
            async with aiofiles.open(readme_path, "w", encoding="utf-8") as f:
                await f.write(remove_markdown_blocks(content))
            print(f"READMEê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {readme_path}")
            await upload_to_s3(BUCKET_NAME, readme_path, readme_key, metadata)
        except Exception as e:
            print(f"README ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    async def process_chunk(self, chunk: str, repo_url: str, prompt: str) -> Optional[str]:
        """ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬"""
        try:
            response = await self.api_client.generate_text_client(prompt, f"git repository url : {repo_url}\n\n" + chunk)
        except Exception as e:
            print(f"Error for chunk")
            return None

        return response


    async def generate_docs(self, directory_path: dict[str, list], 
                            output_directory: str, korean: bool, clone_dir: str):
        """ë¬¸ì„œë§Œ ìƒì„±"""
        try:
            # í•œêµ­ì–´/ì˜ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì •ì˜
            prompts = {
                'Controller': NEW_PROMPT_ARCHITECTURE_DOC_KOREAN if korean else NEW_PROMPT_ARCHITECTURE_DOC,
                'Test': NEW_PROMPT_TEST_DOC_KOREAN if korean else NEW_PROMPT_TEST_DOC
            }
            all_tasks = []
            for category, files in directory_path.items():
                # Controllerì™€ Test ì¹´í…Œê³ ë¦¬ë§Œ ì²˜ë¦¬
                if category not in prompts:
                    continue

                code_contents = self._get_code_contents(files)
                all_tasks.extend([
                    (category, filename, content)
                    for filename, content in zip(files, code_contents)
                ])

            chunk_size = 30
            print("generate docs task size: ", len(all_tasks))
            for i in range(0, len(all_tasks), chunk_size):
                chunk = all_tasks[i:i + chunk_size]
                tasks = [
                    self.send_request_with_client(category, prompts[category], content, filename, clone_dir)
                    for category, filename, content in chunk
                ]
                await asyncio.gather(*tasks)
                await asyncio.sleep(1)  # ë°°ì¹˜ ê°„ ë”œë ˆì´ ì¶”ê°€
            return output_directory

        except Exception as e:
            print(f"ë¬¸ì„œ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return None

    def categorize_files(self, directory):
        """ê° ì¹´í…Œê³ ë¦¬ í´ë”(Service, Controller, Test) ë‚´ì˜ íŒŒì¼ë“¤ì„ ë¶„ë¥˜"""
        categories = {
            "Controller": [],  # Controllerì™€ RestControllerë¥¼ í•¨ê»˜ ì²˜ë¦¬,
            "Test": []
        }

        # ê° ì¹´í…Œê³ ë¦¬ í´ë” í™•ì¸
        for category in ["Controller", "Test"]:
            category_path = os.path.join(directory, category)
            if os.path.exists(category_path):
                for filename in os.listdir(category_path):
                    if filename.endswith(".md"):
                        categories[category].append(filename)

        return categories

    def read_file_content(self, file_path):
        with open(file_path, 'r') as file:
            return file.read()

    def extract_imported_classes(self, content):
        # Regex to find import statements
        import_pattern = r'import\s+([\w\.]+);'
        matches = re.findall(import_pattern, content)
        return matches

    async def summarize_docs_async_nogenerate(self, directory: str) -> None:
        print("SUMMARIZE_DOCS_ASYNC_with_no_generate")

        category_files = self.categorize_files(directory)
        print(category_files)

        for category in ["Controller", "Test"]:
            category_dir = os.path.join(directory, category)
            if not os.path.exists(category_dir):
                print(f"ë””ë ‰í† ë¦¬ ì—†ìŒ: {category_dir}")
                continue

            summary_content = ""
            for filename in category_files.get(category, []):
                file_path = os.path.join(category_dir, filename)
                try:
                    async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
                        content = await f.read()
                        # '## SUMMARY' ì´í›„ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•˜ëŠ” ì •ê·œ í‘œí˜„ì‹
                        match = re.search(
                            r"## SUMMARY\s*(.*)", content, re.DOTALL)
                        if match:
                            summary_content += f"## {filename}\n"
                            summary_content += match.group(1).strip() + "\n\n"
                except Exception as e:
                    print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {file_path}, {str(e)}")
                    continue

            # ì¶”ì¶œëœ ë‚´ìš©ì„ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ summary.md íŒŒì¼ì— ì €ì¥
            output_path = os.path.join(
                directory, f"{category}_summary.md")
            try:
                async with aiofiles.open(output_path, "w", encoding="utf-8") as f:
                    await f.write(remove_markdown_blocks(summary_content))
                print(f"{category}_summary.md íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_path}")
            except Exception as e:
                print(f"{category}_summary.md íŒŒì¼ ìƒì„± ì˜¤ë¥˜: {str(e)}")

    async def send_request_with_client(self, category, prompt, content, 
                                    filename, clone_dir):

        response = await self.api_client.generate_text_client(prompt, content)

                        # íŒŒì¼ ì´ë¦„ ì¶”ì¶œ
        file_name_only = os.path.basename(filename).replace(".java", ".md")
                        # íŒŒì¼ ì´ë¦„ ìƒì„± ë° ì €ì¥
        md_filename = os.path.join(clone_dir, "dododocs", category, f"{file_name_only}")
        os.makedirs(os.path.dirname(md_filename), exist_ok=True)

        async with aiofiles.open(md_filename, "w", encoding="utf-8") as file:
            await file.write(remove_markdown_blocks(response))