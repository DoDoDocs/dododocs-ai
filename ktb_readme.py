from ktb_prompts import *
from ktb_settings import *
from ktb_docs import *

import re
import os
from typing import List, Dict, Optional, Tuple, Any, Generator
import asyncio
import aiohttp
import tiktoken
from PIL import Image
import requests
import io
import aiofiles
from concurrent.futures import ProcessPoolExecutor

"""**async gpt**"""
async def generate_text_async(session, prompt, contents):
    try:
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        json_data = get_json_data(chat_format(prompt, contents))

        async with session.post("https://api.openai.com/v1/chat/completions", json=json_data, headers=headers) as response:
            response_data = await response.json()
            
            if 'error' in response_data:
                print("API ì˜¤ë¥˜:", response_data['error'])
                return None
                
            return response_data['choices'][0]['message']['content']

    except Exception as e:
        print(f"Error summarizing text: {e}")
        return None


async def generate_docs_async(directory_path, output_directory):
    """íŒŒì¼ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¹„ë™ê¸°ë¡œ ìš”ì•½"""
    service_code_contents = get_code_extention(directory_path)  # íŒŒì¼ ì´ë¦„ê³¼ ë‚´ìš©ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ë‹¤ê³  ê°€ì •
    async with aiohttp.ClientSession() as session:
        # ê° íŒŒì¼ ë‚´ìš©ì„ ë¹„ë™ê¸°ë¡œ ìš”ì•½ ìƒì„±
        tasks = [
            generate_text_async(session, GENERATE_DOC_PROMPT, code_content) 
            for code_content in service_code_contents
        ]

        summaries = await asyncio.gather(*tasks)  # ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼

        # íŒŒì¼ëª…ê³¼ ìš”ì•½ì„ ìŒìœ¼ë¡œ ë¬¶ì–´ ë°˜ë³µ
        for filename, summary in zip(directory_path, summaries):
            # íŒŒì¼ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ ê²°ì •
            if "Service" in filename:
                category = "Service"
            elif "RestController" in filename:
                category = "RestController"
            elif "Controller" in filename:
                category = "Controller"
            else:
                category = "Others"
            
            # ì¹´í…Œê³ ë¦¬ í´ë” ê²½ë¡œ ìƒì„±
            category_dir = os.path.join(output_directory, category)
            os.makedirs(category_dir, exist_ok=True)
            
            # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ ì„¤ì •
            output_file_name = os.path.join(category_dir, extract_filename(filename).replace('.java', '.md'))
            
            # íŒŒì¼ ì €ì¥
            with open(output_file_name, "w", encoding="utf-8") as file:
                file.write(summary)

    return None


def categorize_files(directory):
    """ê° ì¹´í…Œê³ ë¦¬ í´ë”(Service, Controller, RestController) ë‚´ì˜ íŒŒì¼ë“¤ì„ ë¶„ë¥˜"""
    categories = {
        "Service": [],
        "Controller": [],
        "RestController": []
    }
    
    # ê° ì¹´í…Œê³ ë¦¬ í´ë” í™•ì¸
    for category in categories.keys():
        category_path = os.path.join(directory, category)
        if os.path.exists(category_path):
            for filename in os.listdir(category_path):
                if filename.endswith(".md"):
                    categories[category].append(filename)
    
    return categories


def find_file_in_directory(directory: str, target_filename: str) -> str:
    """ë””ë ‰í† ë¦¬ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ì—¬ íŒŒì¼ì˜ ì „ì²´ ê²½ë¡œë¥¼ ë°˜í™˜"""
    for root, _, files in os.walk(directory):
        if target_filename in files:
            return os.path.join(root, target_filename)
    return None


async def summarize_docs(files, directory, session):
    if not files:  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²´í¬
        print(f"No {directory} provided to process")
        return {}
    tasks = []
    
    for filename in files:
        # ì „ì²´ íŒŒì¼ ê²½ë¡œ êµ¬ì„±
        file_path = os.path.join(directory, filename)
        
        try:        
            # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ê³  ë””ë ‰í† ë¦¬ê°€ ì•„ë‹Œì§€ í™•ì¸
            if not os.path.isfile(file_path):
                print(f"Skipping directory or non-existent file: {file_path}")
                continue
                
            async with aiofiles.open(file_path, "r", encoding="utf-8") as file:
                text = await file.read()
                tasks.append((filename, generate_text_async(session, SUMMARY_PROMPT, text)))
        except Exception as e:
            print(f"Error reading file {file_path}: {e}")
            continue
    
    if not tasks:
        print(f"No valid files found in directory: {directory}")
        return {}
    
    try:
        # Run tasks and gather results
        results = await asyncio.gather(*[task[1] for task in tasks])
        
        # Build summaries dictionary for the category
        summaries = {tasks[i][0]: results[i] for i in range(len(tasks)) if results[i] is not None}
        return summaries
    except Exception as e:
        print(f"Error processing tasks: {e}")
        return {}

def summarize_category(files, category, directory):
    # ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë³„ë„ì˜ aiohttp.ClientSession ìƒì„±
    async def run():
        async with aiohttp.ClientSession() as session:
            return await summarize_docs(files, os.path.join(directory, category), session)
    
    return asyncio.run(run())


async def summarize_docs_async(directory):
    category_files = categorize_files(directory)
    summaries = { "Service": {}, "Controller": {}, "RestController": {} }
    
    # ProcessPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë©€í‹°í”„ë¡œì„¸ì‹± ì ìš©
    with ProcessPoolExecutor() as executor:
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(
                executor,
                summarize_category,  # ê° ì¹´í…Œê³ ë¦¬ ìš”ì•½ì„ ë³„ë„ì˜ í”„ë¡œì„¸ìŠ¤ì—ì„œ ì‹¤í–‰
                files, category, directory
            )
            for category, files in category_files.items()
        ]
        
        results = await asyncio.gather(*tasks)

        # ê²°ê³¼ë¥¼ summariesì— ì €ì¥
        for category, result in zip(category_files.keys(), results):
            summaries[category] = result

    # Write summaries to markdown files asynchronously
    for category, summary_dict in summaries.items():
        output_path = os.path.join(directory, f"{category}_summary.md")
        
        async with aiofiles.open(output_path, "w", encoding="utf-8") as f:
            await f.write(f"# {category} Files Summary\n\n")
            for filename, summary in summary_dict.items():
                if summary is not None:
                    await f.write(f"## {filename}\n{summary}\n\n")
        
        print(f"{output_path} file created successfully.")
    
    return None


"""GENERATE README"""
def count_tokens(text: str) -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    encoding = tiktoken.encoding_for_model(MODEL)

    # ëª¨ë“  íŠ¹ìˆ˜ í† í°ì„ í—ˆìš©
    return len(encoding.encode(text, disallowed_special=()))


def split_context(text: str, max_tokens: int) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€ í† í° ìˆ˜ì— ë§ì¶° ë¶„í• í•©ë‹ˆë‹¤."""
    encoding = tiktoken.encoding_for_model(MODEL)
    # ëª¨ë“  íŠ¹ìˆ˜ í† í°ì„ í—ˆìš©
    tokens = encoding.encode(text, disallowed_special=())

    chunks = []
    current_chunk = []
    current_length = 0

    for token in tokens:
        if current_length + 1 > max_tokens:
            chunks.append(encoding.decode(current_chunk))
            current_chunk = []
            current_length = 0

        current_chunk.append(token)
        current_length += 1

    if current_chunk:
        chunks.append(encoding.decode(current_chunk))

    return chunks


def get_source_files(repo_dir: str) -> List[str]:
    """Finds and returns source code build files in the given repository directory."""
    source_files = []
    
    for root, dirs, files in os.walk(repo_dir):
        # Exclude specified directories from the search
        dirs[:] = [d for d in dirs if not any(excl in d for excl in exclude_dirs)]
        
        # Check for specific file names
        for file in files:
            # íŒŒì¼ëª…ì´ exclude_dirsì— í¬í•¨ëœ ë¬¸ìì—´ì„ ê°€ì§€ê³  ìˆì§€ ì•Šê³ ,
            # build_file_names ì¤‘ í•˜ë‚˜ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ í¬í•¨
            if (not any(excl in file for excl in exclude_dirs) and 
                any(name in file for name in build_file_names)):
                source_files.append(os.path.join(root, file))
    
    return source_files

async def summarize_chunks_batched(chunks, summary_prompt, session, max_tokens_per_batch=1500000):
    all_summaries = []
    batch = []
    batch_token_count = 0
    print("len chunks : ",len(chunks))

    for chunk in chunks:
        chunk_tokens = count_tokens(chunk)
        print("chunk_tokens : ", chunk_tokens)
        if batch_token_count + chunk_tokens > max_tokens_per_batch:
            # Process the current batch if adding this chunk would exceed the limit
            print("tokens : ", batch_token_count)
            batch_summaries = await asyncio.gather(
                *[generate_text_async(session, summary_prompt, c) for c in batch]
            )
            all_summaries.extend(batch_summaries)
            print("ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ. 1ë¶„ ëŒ€ê¸° ì¤‘...")
            await asyncio.sleep(40)
            batch = []  # Reset the batch
            batch_token_count = 0

        batch.append(chunk)
        batch_token_count += chunk_tokens

    # Process any remaining chunks in the last batch
    if batch:
        batch_summaries = await asyncio.gather(
            *[generate_text_async(session, summary_prompt, c) for c in batch]
        )
        all_summaries.extend(batch_summaries)

    return all_summaries


async def generate_readme(repo_url: str, clone_dir: str, max_tokens: int = MAX_TOKEN_LENGTH) -> Optional[str]:
    """Git ì €ì¥ì†Œë¥¼ ë¶„ì„í•˜ì—¬ README.mdë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        source_files = get_source_files(clone_dir)

        # ì†ŒìŠ¤ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = ""
        for file_path in source_files:
            rel_path = os.path.relpath(file_path, clone_dir)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # íŒŒì¼ êµ¬ë¶„ìë¥¼ ëª…í™•í•˜ê²Œ í•˜ì—¬ íŠ¹ìˆ˜ ë¬¸ì ì¶©ëŒ ë°©ì§€
                    context += f"\n\nFILE_START: {rel_path}\n{content}\nFILE_END\n"
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({rel_path}): {str(e)}")
                continue

        token_count = count_tokens(context)
        print(f"Total src files tokens: {token_count}")
        messages = [{"role": "system", "content": PROMPT_README}]

        if token_count > max_tokens:
            chunks = split_context(context, max_tokens)
            print(f"Split into {len(chunks)} chunks")
            
            # aiohttp ì„¸ì…˜ ìƒì„± ë° ì „ë‹¬
            async with aiohttp.ClientSession() as session:
                chunk_summaries = await summarize_chunks_batched(chunks, PROMPT_README, session)
                print("chunk_summaries length: ", len(chunk_summaries))
                for i, chunk_summary in enumerate(chunk_summaries, 1):
                    messages.append({"role": "user", "content": chunk_summary})
                    print(f"### íŒŒíŠ¸ {i} ë¶„ì„, ê¸¸ì´ : {len(chunk_summary)} í† í°")
                messages.append({"role": "user", "content": f"Create a readme based on the previous conversation history. git repository url : {repo_url}"})
                DOC_RESPONSE, _ = get_completion(messages)
                return DOC_RESPONSE
        else:
            messages.append({"role": "user", "content": context})
            messages.append({"role": "user", "content": f"git repository url : {repo_url}"})
            DOC_RESPONSE, _ = get_completion(
                  messages
                  )
            return DOC_RESPONSE

    except Exception as e:
        print(f"README ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


def get_build_files(repo_dir: str) -> List[str]:
    """Finds and returns source code build files in the given repository directory."""
    source_files = []
    
    for root, dirs, files in os.walk(repo_dir):
        # Exclude specified directories from the search
        dirs[:] = [d for d in dirs if not any(excl in d for excl in exclude_dirs)]
        
        # Check for specific file names
        for file in files:
            # íŒŒì¼ëª…ì´ exclude_dirsì— í¬í•¨ëœ ë¬¸ìì—´ì„ ê°€ì§€ê³  ìˆì§€ ì•Šê³ ,
            # build_file_names ì¤‘ í•˜ë‚˜ì™€ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ í¬í•¨
            if (not any(excl in file for excl in exclude_dirs) and 
                any(name in file for name in build_file_names)):
                source_files.append(os.path.join(root, file))
    
    return source_files


def update_readme_with_usage(readme_content: str, usage_content: str) -> str:
    try:
        # usage_contentì—ì„œ Getting Started ì´í›„ ë‚´ìš©ë§Œ ì¶”ì¶œ
        usage_match = re.search(r"## ğŸš€ Getting Started\n([\s\S]*$)", usage_content)
        if usage_match:
            usage_after_started = usage_match.group(1)  # Getting Started ì´í›„ ë‚´ìš©ë§Œ ê°€ì ¸ì˜¤ê¸°
            
            # readme_contentì—ì„œ Getting Started ì´í›„ ë‚´ìš© êµì²´
            pattern = r"(## ğŸš€ Getting Started\n)([\s\S]*$)"
            new_content = re.sub(pattern, 
                               r"\1" + usage_after_started,
                               readme_content)
            print("USAGEê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return new_content
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
    return readme_content  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜


async def generate_usage(repo_url: str, clone_dir: str, max_tokens: int = MAX_TOKEN_LENGTH) -> Optional[str]:
    """Git ì €ì¥ì†Œë¥¼ ë¶„ì„í•˜ì—¬ README.mdë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        source_files = get_build_files(clone_dir)
        # ì†ŒìŠ¤ ì½”ë“œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = ""
        for file_path in source_files:
            rel_path = os.path.relpath(file_path, clone_dir)
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # íŒŒì¼ êµ¬ë¶„ìë¥¼ ëª…í™•í•˜ê²Œ í•˜ì—¬ íŠ¹ìˆ˜ ë¬¸ì ì¶©ëŒ ë°©ì§€
                    context += f"\n\nFILE_START: {rel_path}\n{content}\nFILE_END\n"
            except Exception as e:
                print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ ({rel_path}): {str(e)}")
                continue

        token_count = count_tokens(context)
        print(f"Total build files tokens: {token_count}")
        messages = [{"role": "system", "content": PROMPT_USAGE}]

        if token_count > max_tokens:
            chunks = split_context(context, max_tokens)
            print(f"Split into {len(chunks)} chunks")
            
            # aiohttp ì„¸ì…˜ ìƒì„± ë° ì „ë‹¬
            async with aiohttp.ClientSession() as session:
                chunk_summaries = await summarize_chunks_batched(chunks, PROMPT_USAGE, session)
                for i, chunk_summary in enumerate(chunk_summaries, 1):
                    messages.append({"role": "user", "content": chunk_summary})
                    print(f"### íŒŒíŠ¸ {i} ë¶„ì„, ê¸¸ì´ : {len(chunk_summary)} í† í°")
                messages.append({"role": "user", "content": f"Create a readme based on the previous conversation history. git repository url : {repo_url}"})
                DOC_RESPONSE, _ = get_completion(messages)
                return DOC_RESPONSE    
        else:
            messages.append({"role": "user", "content": context})
            messages.append({"role": "user", "content": f"git repo url : {repo_url}"})
            DOC_RESPONSE, _ = get_completion(
                  messages
                  )
            return DOC_RESPONSE

    except Exception as e:
        print(f"USAGE ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None


def read_description_from_readme(file_path="README.md"):
    try:
        with open(file_path, "r", encoding="utf-8") as file:
            content = file.read()
            # Description ì„¹ì…˜ì„ ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ
            description_match = re.search(r"Overview\n(.*?)(### Main Purpose)", content, re.S)

            if description_match:
                return description_match.group(1).strip()
            else:
                raise ValueError("Could not find 'Overview' or 'Core Technology Stack' sections in the README.md")
    except FileNotFoundError:
        raise FileNotFoundError(f"File '{file_path}' does not exist.")

# OpenAI APIë¥¼ ì‚¬ìš©í•´ DALL-Eë¡œ ì´ë¯¸ì§€ ìƒì„±
def generate_image(description, clone_dir, new_size=(400, 400)):
    response = client_gpt.images.generate(
      model="dall-e-3",
      prompt=DALLE_PROMPT+description,
      size="1024x1024",
      quality="standard",
      n=1,
    )
    image_url = response.data[0].url
    image_data = requests.get(image_url).content
    
    img = Image.open(io.BytesIO(image_data))
    resized_img = img.resize(new_size)

    resized_img.save(clone_dir+"/"+"generated_image.png")

    return image_url, clone_dir+"/"+"generated_image.png"


def update_readme_with_image(file_path="README.md", image_path="./generated_image.png"):
    with open(file_path, "r+", encoding="utf-8") as file:
        content = file.read()

        # 'Preview' ì„¹ì…˜ ë’¤ì— ì´ë¯¸ì§€ë¥¼ ì‚½ì…
        new_content = re.sub(r"(Preview\n)(.*?)(##|$)",
                             f"Preview\n\n<img src='{image_path}' width='400' height='400'/>\n\n\\3",
                             content, flags=re.S)

        # íŒŒì¼ì— ë³€ê²½ ì‚¬í•­ ì“°ê¸°
        file.seek(0)
        file.write(new_content)
        file.truncate()
